# -*- coding: utf-8 -*-
"""casting-Surface_Alexnet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1a7kuuytgreO9_-yfsBDjTIpkoP3T9XKS
"""

from google.colab import drive
drive.mount('/content/gdrive/')

!cp -r '/content/gdrive/MyDrive/datasets/casting_surface_final.zip' '/content/'

!unzip \*.zip && rm *.zip

#Imports
import numpy as np
import os
import PIL
import PIL.Image
import tensorflow as tf
import tensorflow_datasets as tfds
import matplotlib.pyplot as plt

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential


from keras.layers import Conv2D,MaxPooling2D,Dense,Flatten,Dropout
from keras.layers.normalization import BatchNormalization

print(len(os.listdir('casting_surface_final/def')))
print(len(os.listdir('casting_surface_final/ok')))

# Loading data
data_dir = '/content/casting_surface_final'
# Create parameters
batch_size = 16
image_height = 200
image_width = 200

# define training set
train_ds = tf.keras.preprocessing.image_dataset_from_directory(
    data_dir,
    validation_split=0.8,
    color_mode="grayscale",
    subset="training",
    seed=123,
    image_size=(image_height, image_width),
    batch_size=batch_size
)

# define validation set
val_ds = tf.keras.preprocessing.image_dataset_from_directory(
  data_dir,
  validation_split=0.8,
  color_mode="grayscale",
  subset="validation",
  seed=123,
  image_size=(image_height, image_width),
  batch_size=batch_size
)

# class names
class_names = train_ds.class_names
print("Classes are:", class_names)

for image_batch, labels_batch in train_ds:
  print(image_batch.shape)
  print(labels_batch.shape)
  break

# configure data set performance to prevent I/O blocking
AUTOTUNE = tf.data.AUTOTUNE
train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)

# AlexNet Model

img_height = image_height
img_width = image_width
num_classes = len(class_names)

model = Sequential([

      # Preprcessing layer
      layers.experimental.preprocessing.Rescaling(1./255, input_shape=(img_height, img_width, 1)),
      
      # 1 convolution layer
      layers.Conv2D(filters=96, 
                    kernel_size=(11, 11), 
                    strides=(4, 4), 
                    padding='same', 
                    activation='relu',
                    input_shape=(img_height, img_width, 1)),
      # 1 Max Pool layer
      layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2)),
      BatchNormalization(),

      # 2 convolution layer
      layers.Conv2D(filters=256, kernel_size=(5, 5), strides=(1, 1), padding='same', activation='relu'),
      # 2 Max Pool layer
      layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2)),
      layers.BatchNormalization(),

      # 3 convolution layer
      layers.Conv2D(filters=384, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu'),

      # 4 Convolution layer
      layers.Conv2D(filters=384, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu'),

      # 5 Convolution layer
      layers.Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu'),

      # 3 Max pool layer
      layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2)),
      layers.BatchNormalization(),
      
      layers.Flatten(),

      # 1 Dense layer
      layers.Dense(4096, activation='relu'),
      layers.Dropout(0.4),
      layers.BatchNormalization(),

      # 2 Dense layer
      layers.Dense(4096, activation='relu'),
      layers.Dropout(0.4),
      layers.BatchNormalization(),

      # 3 Dense layer
      layers.Dense(1000, activation='relu'),
      layers.Dropout(0.4),
      layers.BatchNormalization(),

      # Output layer
      layers.Dense(num_classes, activation='softmax')
])

model.summary()

# Compile
model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

epochs=10
history = model.fit(
  train_ds,
  validation_data=val_ds,
  epochs=epochs
)

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(epochs)

print("Training accuracy:", acc[epochs-1])
print("Training loss:", loss[epochs-1])
print("Validation accuracy:", val_acc[epochs-1])
print("Validation loss:", val_loss[epochs-1])


plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

# Implementing